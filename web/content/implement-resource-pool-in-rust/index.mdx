---
title: Resource Pooling in Rust
publishedDate: 2024-09-15
image: "../../public/images/image4.jpg"
isPublished: true
description: "Not init on the fly, cached but not too much instance, and not blocking the entire request when reached pool max. It is how we use and maintain our Http Connection, Web Socket, Database, gRPC connection"
authorDisplayName: "Tien Dang"
authorEmail: "tiendvlp@gmail.com"
authorFullName: "Dang Minh Tien"
keywords:
  - Rust
  - Design pattern
  - Resource pooling
  - Object pooling
---

# Why we write this article ?

During software development, there are so much problems that we need to resolved, and we don't want to depending on some random libraries. There might be some design patterns on the internet, but it just not working,
because it's implementation is so much simple to be used in practice.

We wan't to share our solutions on every small problems that we have. So that you might don't need to think about why

# The Problems

For any application to be able to run seamlessly over time, there are a lot of aspects that we have to cover, from memory safety to infrastructure. In this article, I would love to share a solution for an application that interacts with a limited source of resources. For example, 10,000 connections is the maximum number of concurrent connections to PostgreSQL, or 1024 is the default limit for concurrent opened files in Ubuntu. In that case, we cannot just ask for more and more resources as needed; we have to have a better strategy.

Moreover, interacting with these resources—whether through networking or mainly CPU-bound operations—produces some overhead if done excessively. Imagine if we make a new connection to the database on every RESTful request. It would be a disaster for our system; we would waste most of our CPU time initiating required resources instead of focusing on the main logic.

If I did touch your pain point(s), welcome to my article. The design pattern we will discuss today is Resource Pooling, using Rust and asynchronous syntax.

# Who Will Benefit Most from Reading This?

- **Engineer who don't want to use library for small problems, this code is ready to used unless you need to add some extra features**
- **Developers looking for an implementation of resource pooling in Rust.**
- **If you are looking for a resource pooling solution that works with asynchronous programming, btw we're using Tokio, I think you just need to update some importes statements if you're not using tokio for asynchorous or you're not using asynchronous at all. **
- **High-expectation developers who not only want a solution but also desire the best experience while using it.**

# The ideas

To start, let’s briefly examine how straightforward it is to use once implemented:

```rust
pub struct UserRepository {
    pub db_request: PoolRequest<DbConnection>
}

impl UserRepository {
    async fn find_all(&self) -> Result<Errors, Vec<User>> {
        let db: Option<PoolResponse<DbConnection>> = self.db_request.retrieive().await;
        if db.is_none() {
            return Err(Errors::DatabaseConnectionError)
        }

        let db = db.unwrap();
        // The PoolResponse has been implement both Deref and DerefMut trait
        // which allow us to access the resource function directly
        let db_results = db.query("SELECT * FROM Users").await;
        // It will automatically return back to the pool after dropped
        drop(db); // Optional
    }
}
```    
The `PoolRequest<T>` is a lightweight object that acts as a bridge to the `Pool`. It is cheap to be cloned across async tasks.

The `retrieve` function will request the pool to provide us a `PoolResponse<T>`.

Then we can use the `PoolResponse<T>` as it is our resource because it can be dereferenced into the resource.

The `PoolResponse` also making sure there are no resources could escape out of the pool, every resource after leave the pool, must return back to the pool to be reused by others.

Take a look at this simple flow on how a basic API application will use the pool to solve resource initialization overhead by applying the Resource Pool pattern
(more complicated examples are provided at the end).

<div className='md:w-[100%]'>
  <Lottie className='aspect-[2/1.8] rounded-lg' file='dotlottie/resource-pool_-_simple-flow.lottie'/>
</div>

When a new request arrives, the request handler initializes a `PoolRequest<T>` that holds a connection to the pool. There’s no need to create a new resource; a `PoolRequest` is a simple struct that can be easily cloned across different asynchronous tasks.
The `PoolRequest` then acquires a resource from the pool for us to use freely, and once we’re finished, the resource automatically returns to the pool to wait for other requests.

# More problems and solutions
But by using the simple approach described above, four potential issues may arise:

#### 1. Inability to provide resources when the number of concurrent requests exceeds the number of resources in the pool.

   <ArrowRight/> To address this, we'll implement two limits. The first is `min_pool_size`, which defines the minimum number of resources that are always ready for use. The second is `max_pool_size`, which can be much larger than `min_pool_size`. This allows additional resources to be initialized to handle extra requests, but the total number of resources will not exceed `max_pool_size`.

#### 2. Applying `max_pool_size` could result in a rapid increase in the number of resources.

   <ArrowRight/> We'll introduce `resource_idle_timeout`, specifying the maximum time an unused resource can remain in memory while still maintaining the minimum number of resources in the pool.

   For example, if your RESTful API most of the time only need to handle 10 concurrent requests and you set `min_pool_size = 10`, but suddenly receive 1,000 requests at once, your application will return to 10 concurrent requests after the surge, leaving 990 resources unused in the pool.

   By setting `resource_idle_timeout = 5 minutes`, the pool can release those 990 unused resources if they remain idle for five minutes.

   <Info>Note: The number of resources in the pool will always be `≥ min_pool_size`.</Info>

#### 3. What if requests exceed `max_pool_size`? Should we fail those requests?

   <ArrowRight/> We can handle this by applying `retrieving_timeout` to `PoolRequest<T>`. This allows the request to wait (within a specified timeout) for a resource to be released back into the pool, which can then be used for the current request.

#### 4. Setting a high `min_pool_size` might cause our service to take a long time to start because it needs to initialize many resources.

   <ArrowRight/> Instead of initializing resources one at a time, we can initialize them concurrently, reducing the startup time.

#### Below is the final diagram illustrating the flow of retrieving, releasing resources, and removing idle resources
<Lottie className='aspect-[1.3/1] rounded-lg' file='dotlottie/resource-pool_-_full-flow.lottie'/>

# Implementation

### Resource Abstraction

Before implementing the pool, we need a way to abstract the resources because the pool doesn't concern itself with what the resource is or how to initialize it. It only cares about the state of the resource—for example, whether it has timed out.

#### a. ResourceProvider

To abstract the process of creating new resources, we'll create a trait that encapsulates the initialization logic for different types of resources.

```rust
pub trait PoolResourceProvider<T>: Send + Sync
    where T: Send + Sync
{
    async fn new(&self) -> T where Self: 'async_trait;
}
```

You may notice that the new function cannot accept custom parameters, but it’s essential to initialize a resource with custom parameters.

No need to worry; we can create a struct to hold the parameters, and that struct will implement the `PoolResourceProvider<T>` trait.

```rust
pub struct DbPoolProvider {
  connection_string: string
}

impl PoolResourceProvider<PosgresConnection> for DbPoolProvider {
  async fn new(&self) -> PosgresConnection {
    let db = postgres.connect(self.connection_string).await;
    db
  }
}
```

Now that we’ve established an effective abstraction for creating new resources, let’s proceed to the next step.

#### b. PoolItem\<T>

The fundamental idea behind a `PoolItem` is that it will manage the state of each resource, for example `How long this resource is being in the pool`.

Also, the `PoolItem` making sure our code will be easy to maintained, provide us a way to implement more detailed logic on how to manage each resource.

Let's take a look on struct `PoolItem<T>`

```rust
pub struct PoolItem<T> where T: Send + Sync + 'static
{
    // Put the resource on Heap could reduce memory usage. For example if there are three variables with type Option<T>,
    // because of how the Stack work,
    // it will allocate three times of memory eventhough the Option is None.
    // By putting the resource on the Heap, each variable only take 8 bytes on Stack.
    resource: Box<T>,
    // Tracking the start time of the resource to calculate the elasped time.
    start_time: Instant,
    // When the resource laying in the pool more than this limit, it will be removed.
    max_idling_timeout: Duration
}
```

#### 2. The `start_time` and `max_idling_timeout`

These are used to calculate the total idle time of a resource. If the resource doesn't belong to the `min_pool_size`, it will be dropped after it exceeds the `max_idling_timeout`.

Next, we will take a look at the implementation of the `PoolItem`:

```rust
impl<T> PoolItem<T> where T: Send + Sync + 'static
{
    pub fn new(resource: Box<T>, max_idling_timeout: Duration) -> Self {
        Self {
            resource,
            start_time: Instant::now(),
            max_idling_timeout
        }
    }

    pub fn refresh(&mut self) {
        self.start_time = Instant::now();
    }

    pub fn timeleft(&self) -> Duration {
       let elapsed = self.start_time.elapsed();
       if elapsed.gt(&self.max_idling_timeout) {
           return Duration::ZERO
       }

       self.max_idling_timeout - elapsed
    }
}
```
There are two functions that we need to talk about:

The `::refresh` function will be called when a `PoolItem` has finished it's duty and returned back to the pool.
We will refresh it's count down to the beginning as a 'award' that it's has finished it's duty instead
of laying in the pool and doing nothing.

The `::timeleft` function will counting the max amount of time that the resource could be laying in the pool and doing nothing,
if it exceed the timeout which means the timeleft will return `Duration::ZERO`, then it will be removed by the `PoolCleaner` we will discuss further later in this post ?

### The Pool

Finally, let's discuss our main component: the pool itself.

The pool consists of three primary parts:

The **Pool**: Hold an array of resources, reponsible for managing the list of resources, and making sure every new resource that be created will also be counted very carefully. This is also the core part which will be shared by `Allocator` and `Cleaner`.

The **Allocator**: This component initializes resources and provides them as needed. It handles the creation of new resources and supplies them to requesters.

The **Cleaner**: This part is responsible for maintaining the pool's efficiency by cleaning up unused resources, preventing unnecessary memory usage.


#### a. Pool\<T>

Let's talk about the core `Pool<T>` first, here is it's structure:

```rust
pub struct Pool<T> where T: Send + Sync + 'static {
    min_size: usize,
    max_size: usize,
    items: Mutex<VecDeque<PoolItem<T>>>,
    counter: Mutex<usize>,
}
```

The `min_size`: Is the minimum amount of resources that will always be in the pool, it help our system reduce overhead in creating 
new resource too freequently.

The `max_size`: Is the maximum amount of resources that could be created by the pool, whenever the pool don't have enough resource to provide, it could create some extra resouces to serve, and after these extra resources is used, it will be cleanup if it not in used for
`max_idling_timeout` as described above.

The `items`: This hold an array of `PoolItem`, it is wrapped in a `Mutex` because the entire struct `Pool<T>` will be 
saved in the heap instead of stack, and will be shared across across different async tasks.

The `counter`: This is used to counting every resource in the pool. You might asked why we are not using the `items.len()` instead.
But it is because a resource when being requested it will be take out of the `items` then eventhough the `items` does reduced it's size
but there are a resource has been created and borrowed, we can not trust the `items.len()` we need to have another counter, and here is it.

Here is it's implementation, noticed that all of the import is from `tokio`.

```rust
impl<T> Pool<T> where T: Send + Sync + 'static {
    fn new(min_size: usize, max_size: usize) -> Self {
        if min_size > max_size {
            panic!("The min size can not be greater than max size");
        }

        Self {
           items: Mutex::new(VecDeque::with_capacity(min_size)),
           counter: Mutex::new(0),
           min_size,
           max_size
        } 
    }

    async fn add_new_item(&self, item: PoolItem<T>) {
        if self.increase_counter().await.is_ok() {
            self.items.lock().await.push_front(item);
        }
    }

    async fn borrow_item(&self) -> Option<PoolItem<T>> {
        self.items.lock().await.pop_front()
    }

    async fn return_borrowed_item(&self, mut item: PoolItem<T>) {
        let mut counter = self.counter.lock().await;

        if *counter <= self.max_size {
            item.borrow_mut().refresh();
            drop(counter);
            self.items.lock().await.push_front(item);
        }
        else {
            *counter -= 1;
            // Discard orphan resource
            // This only occured when there is an item
            // that not belong to this pool.
        }
    }

    async fn increase_counter(&self) -> Result<usize, usize> {
        let mut counter = self.counter.lock().await;

        if *counter < self.max_size {
            *counter += 1;
            return Ok(*counter)
        }

        Err(*counter)
    }

    async fn decrease_counter(&self) -> Result<usize, usize> {
        let mut counter = self.counter.lock().await;
        if *counter > self.min_size {
            *counter -= 1;
            return Ok(*counter);
        }

        Err(*counter)
    }

    async fn invalidate(&self, index: usize) -> Result<(), Duration> {
        let mut items = self.items.lock().await;
        let item = items.get(index);
        if item.is_none() {
            return Ok(())
        }

        let timeleft = item.as_ref().unwrap().timeleft();
        if !timeleft.is_zero() {
            return Err(timeleft)
        }

        if let Some(removed_item) = items.swap_remove_back(index) {
            drop(items);
            if let Err(_) = self.decrease_counter().await {
                self.items.lock().await.push_back(removed_item);
            }
        }

        Ok(())
    }

    async fn number_of_available_items(&self) -> usize {
        self.items.lock().await.len()
    }

    async fn is_exceed_min_size(&self) -> bool {
        *self.counter.lock().await > self.min_size
    }

    // Wait until no requests to the pool and the cleanup has finished it's job
    pub async fn wait_for_idle(&self) {
        loop {
            if self.min_size == *self.counter.lock().await && 
               self.min_size == self.number_of_available_items().await
            {
                break;
            }

            yield_now().await;
        }
    }
}
```

The `invalidate` function will verify an item at `index` to check if it belong to an extra pool or not, and if it does then is it
exceeded the pool timeout which means it's has laying and doing nothing for too long, if it does then it will be removed right away.
The counter also need to be reduced.

#### a. PoolAllocator\<T>

First let's take a look on the struct.

```rust
pub struct PoolAllocator<T>
where
    T: 'static + Send + Sync
{
    pool: Arc<RwLock<Vec<PoolItem<T>>>>,
    min_pool_size: usize,
    max_pool_size: usize,
    resource_idle_timeout: Duration,
    resource_provider: Box<dyn PoolResourceProvider<T> + 'static>,
    pool_cleaner: PoolCleaner<T>
}
```

We create a `PoolAllocator` struct that requires a generic type `<T>`, representing the type of resource it will provide.

The `PoolAllocator<T>` holds a `Vector` of `PoolItem<T>`, which is wrapped inside an `Arc` to allow sharing the reference across different threads. We then wrap this with an `RwLock`, enabling multiple readers but only one writer at a time. Together, these wrappers allow us to share the `PoolAllocator` across threads without violating Rust's memory safety rules.

Regarding why the allocator needs to keep a reference to the `PoolCleaner`, it's because it decides when to start the cleanup.
Additionally, this ensures that the `PoolCleaner<T>` will be dropped when the allocator is dropped.

Now, let's examine the main logic of the allocator to see how it works.

```rust
impl<T> PoolAllocator<T> where T: Send + Sync + 'static
{
    pub async fn init(&mut self)
    {
        tokio_scoped::scope(|scope| {
            for _i in 0..self.min_pool_size {
                let pool = self.pool.clone();
                let resource_provider = &self.resource_provider;
                scope.spawn(async move {
                    let resource = resource_provider.new().await;
                    let pool_item: PoolItem<T> = PoolItem::new(resource, None).await;

                    let mut pool = pool.write().await;
                    pool.push(pool_item);
                });
            }
        });

        let pool = self.pool.read().await;
        info!(
            target: "pool-allocator",
            "Initialized {} resources to fit the min_pool_size={}",
            pool.len(),
            self.min_pool_size
        );

        self.pool_cleaner.start_background();
    }

    pub async fn len(&self) -> usize {
        let pool = self.pool.read().await;
        pool.len()
    }

    pub async fn retrieve(&self) -> Option<T> {
         let mut pool = self.pool.write().await;
         let size = pool.len();

         let found_pool_item = pool.iter_mut().find(|it| !it.is_empty());
         if found_pool_item.is_some() {
            return found_pool_item.unwrap().take()
         }
         else if size < self.max_pool_size as usize {
            let resource = self.resource_provider.new().await;
            let mut new_item = PoolItem::<T>::new(
                resource,
                Some(self.resource_idle_timeout))
                .await;

            let resource = new_item.take();
            pool.push(new_item);

            return resource
         }

         None
    }

    pub async fn put(&self, resource: T) {
        let mut pool = self.pool.write().await;
        let found_empty_item = pool.iter_mut().find(|it| it.is_empty());
        if found_empty_item.is_some() {
            found_empty_item.unwrap().put(resource);
        }
        else {
            // Every resource when complete, it will always have place to return to the pool
            error!(target: "pool-allocator", "Found an orphan resource, dropping it...");
         }
    }
}
````

The `::init` function has two main responsibilities. First, it initializes the minimum number of resources to populate the pool. And to prevent long startup time
we will create all resources concurrently instead of one by one, and notice that these minimum resources will not have `max_idling_timeout`.
Second, it initiates the cleanup process, which runs in the background to monitor the pool and remove unused resources.

The `::retrieve` function extracts a resource from the pool. If no resource is available, it checks the current number of resources in the pool. If this number is still less than the `max_pool_size`, the allocator will initialize additional resources, and these extra resources will has `max_idling_timeout`.
Finally, the new resource will be used to serve the request.

The `::put` function returns the resource to the pool. It's important to note that the pool will only accept the resource if there is an available slot (`PoolItem.resource = None`). And if no available slot is found, the resource will be dropped, and it will never happen, unless there is a resource that not comming from the pool.

#### b. PoolCleaner\<T>

We have successfully implemented resource allocation; now it's time to free up resources after they've been used.
To clean up unused resources, we track the idle time of any resource that doesn't belong to the `min_pool`. 
If a resource's idle time exceeds the `resource_idle_timeout`, we remove it.

<Info>
  A resource is considered idle when it isn't being requested and only laying within the `PoolItem`.
</Info>

Let's take a quick look at how this process works:

```rust
pub enum CleanupStrategy {
    Relax { interval: Duration }
}

pub struct PoolCleaner<T>
where
    T: Send + Sync + 'static
{
    strategy: CleanupStrategy,
    pool: Arc<RwLock<Vec<PoolItem<T>>>>,
    background_handler: Option<JoinHandle<()>>
}
```


The CleanupStrategy enum exists because cleaning up resources on time can be challenging and may require high CPU usage.
Therefore, it’s better to apply different approaches for different types of resources. 
Currently, I only only propose one simple cleanup approach called Relax.
In this mode, we start a background job that performs the cleanup task at each specified interval. 
It’s called Relax because it might not clean up resources exactly on time and may have a slight delay.
You might want to consider another approach to suit your specific cases.

The implementation

```rust
impl<T> PoolCleaner<T>
where
    T: Send + Sync + 'static,
{
    pub fn new(pool: Arc<RwLock<Vec<PoolItem<T>>>>, strategy: CleanupStrategy) -> Self {
        Self {
            strategy,
            pool,
            background_handler: None
        }
    }

    pub fn start_background(&mut self) {
        match self.strategy {
            CleanupStrategy::Relax {
                interval
            } => {
                let interval = interval.clone();
                let pool = self.pool.clone();
                let handler = spawn(async move {
                    loop {
                        sleep(interval).await;
                        Self::request_cleanup_loop(&pool).await
                    }
                });

                self.background_handler = Some(handler);
            }
            _ => {
                // Not support for other task
            }
        }
    }

    pub fn stop_background(&mut self) {
        if self.background_handler.is_some() {
            self.background_handler.take().unwrap().abort();
        }
    }

    pub async fn request_cleanup_loop(pool_sync: &Arc<RwLock<Vec<PoolItem<T>>>>) {
        let mut pool = pool_sync.write().await;

        pool.retain_mut(|it| !it.invalidate());
    }
}

impl<T> Drop for PoolCleaner<T>
where
    T: Send + Sync + 'static,
{
    fn drop(&mut self) {
        self.stop_background();
    }
}
```

Let's begin with the key functionality: the `request_cleanup_loop` function. This function calls `invalidate` on each item, allowing them to determine how to clean up their resource.
The returned value, `true` or `false`, informs the cleaner whether an item has completed its cleanup and can be removed from the pool.

The `start_background` function is triggered by the pool within the `init` function. If you'd prefer to use a cleanup strategy that doesn't require a background handler, you can simply leave the match arm empty within this function.

### The PoolRequest\<T>

The `PoolAllocator` is great, but it doesn’t seem as great when we have to share its reference across various async tasks and modules.
We all know how tough Rust can be, right?

That’s where the `PoolRequest` comes in to help us manage Rust’s memory safety rules.

Additionally, the PoolRequest will handle the logic of waiting for available resources when the PoolAllocator has none left.

Let’s take a look at how it works:

```rust
pub struct PoolRequest<T>
where
    T: Send + Sync + 'static,
{
    retrieving_timeout: Option<Duration>,
    pool: Arc<PoolAllocator<T>>
}
```

First, we need to agree that in order to share the same `PoolAllocator` instance across multiple async tasks, we have to place it on the heap instead of the stack.
Then, we wrap it in an `Arc`, which allows us to clone and share the instance while ensuring thread safety across different threads.

Now, let's move on to its implementation:

```rust
impl<T> PoolRequest<T>
where
    T: Send + Sync + 'static
{
    pub async fn retrieve(&self) -> Option<PoolResponse<T>> {
        let resource = self.pool.retrieve().await;

        if resource.is_none() {
            if self.retrieving_timeout.is_none() {
                return None
            }

            let now = Instant::now();
            let mut elapsed = Duration::new(0, 0);

            while elapsed.lt(self.retrieving_timeout.as_ref().unwrap()) {
                let resource = self.pool.retrieve().await;
                if resource.is_some() {
                    return Some(PoolResponse::new(resource.unwrap(), self.pool.clone()));
                }

                elapsed = now.elapsed();
            }

            return None
        }

        let response = PoolResponse::new(resource.unwrap(), self.pool.clone());

        Some(response)
    }
}
```

What the `retrieve` function is doing is that it will call the `take` function from the pool, and if there is no available resource, it will create a loop to continuously asking the resource
from the pool within the `retrieving_timeout`.

There is not much thing to talk about this `retrieve` function, let's explain why we need to return the `PoolResponse` instead of a resource.

### The PoolResponse\<T>

You might ask, why do we have to use it? 
To be honest, I would prefer the `PoolRequest` to return the resource directly.
We all know how it feels to have ownership of an instance rather than just a reference, right?

But every resource has to go back to the pool to be reused by the others ! What if I, you, or one of our colleagues takes the resource and just drops it? It would never return to the pool, and it would never be reused as it should be.
We need a better approach. The `PoolResponse` ensures the resource is always returned to the pool.

Now, let's take a look at how it works.

```rust
pub struct PoolResponse<T> where T: Send + Sync + 'static
{
    resource: Option<T>,
    pool: Option<Arc<PoolAllocator<T>>>
}
```

It's easy to understand why the `PoolResponse` needs to hold a reference to the `PoolAllocator`: Because it must know which pool it need to return back after use.

Another thing is that both the resource and pool variables are Option; both are required to be `Some(thing)` when initialized.
We’ll understand how could it become `None` in this implementation.

```rust
impl<T> PoolResponse<T> where T: Send + Sync + 'static
{
    pub fn new(resource: T, pool: Arc<PoolAllocator<T>>) -> Self {
        Self {
            resource: Some(resource),
            pool: Some(pool)
        }
    }
}

impl<T> Deref for PoolResponse<T> where T: Send + Sync + 'static,
{
    type Target = T;

    fn deref(&self) -> &Self::Target {
        self.resource.as_ref().expect("Cannot access returned resource")
    }
}

impl<T> DerefMut for PoolResponse<T> where T: Send + Sync + 'static
{
    fn deref_mut(&mut self) -> &mut Self::Target {
        self.resource.as_mut().expect("Cannnot access the returned resource")
    }
}

impl<T> Drop for PoolResponse<T> where T: Send + Sync + 'static
{
    fn drop(&mut self) {
        let pool = self.pool.take();
        let resource = self.resource.take();

        spawn(async move {
            pool.expect("This response already dropped")
                .put(resource.expect("The response already dropped"))
                .await;
        });
    }
}
```

As I already mentioned above, the `PoolResponse::new` function will not accept `None` from either the resource or the pool.

By implementing both `Deref` and `DerefMut`, we are able to freely interact with the resource without any limitations. We can also call `expect` without any hesitation because it cannot fail; the pool and resource will only be `None` when it is dropped.

And the secret on how the resource could automatically return to the pool after used is within the `Drop` trait implementation. Whenever the `PoolResponse` goes out of scope or we manually call `drop(response)`, we will spawn an async task that returns the resource back to the pool. This also explains why the pool and resource need to be an `Option`, because we need to take them out and pass them to the async task, which runs independently after the `PoolResponse` is dropped.

# Bonus: The pool builder

Even with this simple design, the `PoolAllocator<T>` required 6 parameters, I think it is too much, you might want to scale it even further, for example, set the maximum number of concurrent resource to be init at the same time.

I think that's enough reason for a builder pattern to be implemented.

This code need to be placed in the same file with the `PoolAllocator` because all properties of `PoolAllocator` need to be private.

```rust
pub struct PoolBuilder<T> where T: Send + Sync + 'static
{
    min_pool_size: usize,
    max_pool_size: usize,
    resource_idle_timeout: Duration,
    pool_resource_provider: Option<Box<dyn PoolResourceProvider<T> + 'static>>,
    cleanup_strategy: Option<CleanupStrategy>
}

impl<T> PoolBuilder<T> where T: Send + Sync + 'static
{
    pub fn new(resource_provider: Box<dyn PoolResourceProvider<T> + 'static>) -> Self {
        Self {
            min_pool_size: 5,
            max_pool_size: 50,
            resource_idle_timeout: Duration::new(10, 0),
            pool_resource_provider: Some(resource_provider),
            cleanup_strategy: Some(CleanupStrategy::Relax {
                interval: Duration::new(10, 0),
            })
        }
    }

    pub fn min_pool_size(mut self, size: usize) -> Self {
        if size <= 0 {
            panic!("The min pool size must be positive");
        }

        self.min_pool_size = size;

        self
    }

    pub fn max_pool_size(mut self, size: usize) -> Self {
        if size <= 0 {
            panic!("The max pool size must be positive");
        }

        self.max_pool_size = size;
        self
    }

    pub fn cleanup(mut self, strategy: CleanupStrategy) -> Self {
        self.cleanup_strategy = Some(strategy);
        self
    }

    pub async fn build(mut self) -> Arc<PoolAllocator<T>>
    {
        if self.max_pool_size < self.min_pool_size {
            panic!("The max pool size can not less than min pool size");
        }

        let pool = Arc::new(RwLock::new(Vec::with_capacity(self.min_pool_size)));

        let cleaner = PoolCleaner::new(pool.clone(), self.cleanup_strategy.take().unwrap());
        let mut pool_allocator = PoolAllocator {
            max_pool_size: self.max_pool_size,
            min_pool_size: self.min_pool_size,
            resource_provider: self.pool_resource_provider.take().expect("The resource_provider is required"),
            resource_idle_timeout: self.resource_idle_timeout,
            pool,
            pool_cleaner: cleaner
        };

        pool_allocator.init().await;

        Arc::new(pool_allocator)
    }
}

```

# The usage

Assuming we have this DbConnection as our resource, and the each query will took `100 nanosecs` to execute
```rust
pub struct DbConnection {}

impl DbConnection {
    pub async fn new(_: String) -> Self {
        Self {}
    }

    pub async fn query(&self) {
        tokio::time::sleep(Duration::new(0, 100)).await;
    }
}
```

Then we abstract the resource by implement `PoolResourceProvider` trait
```rust
pub struct DbPoolResourceProvider {
    connection_string: String
}

#[async_trait::async_trait]
impl PoolResourceProvider<DbConnection> for DbPoolResourceProvider {
    async fn new(&self) -> DbConnection {
       DbConnection::new(self.connection_string.clone()).await
    }
}
```

Now let's test it ! In this test, I created `10,000` tasks, they will together request a new resource from the pool concurrently.

```bash
min_pool_size: 10
max_pool_size: 20
retrieving_timeout: 1 seconds
resource_idle_timeout: 100 nanosecs
```

With these configurations, we expected the 

```rust
// Keep the pool as static
pub static DB_POOL: OnceCell<Arc<PoolAllocator<DbConnection>>> = OnceCell::const_new();

#[tokio::test(flavor = "multi_thread")]
pub async fn test_pool() {
    DB_POOL.get_or_init(|| async move {
        let resource_provider = DbPoolResourceProvider {
            connection_string: String::from("mydb://root@root")
        };

        let pool = PoolBuilder::new(Box::new(resource_provider))
            .min_pool_size(10)
            .max_pool_size(20)
            .resource_idle_timeout(Duration::new(0, 100))
            .build().await;

        Arc::new(pool)
    }).await;

    let request = PoolRequest {
        pool: DB_POOL.get().unwrap().clone(),
        retrieving_timeout: Some(Duration::new(1, 0))
    };

    let number_of_tasks: usize = 10000;
    let success_counter: Arc<RwLock<i32>> = Arc::new(RwLock::new(0));

    tokio_scoped::scope(|scope| {
        for _ in 0..number_of_tasks {
            // simply clone the request and pass the ownership to any other threads.
            let request = request.clone();
            let success_counter = success_counter.clone();

            scope.spawn(async move {
                let db = request.retrieve().await;
                if db.is_none() {
                    return
                }

                let db = db.unwrap();
                db.query().await;

                let mut success_counter = success_counter.write().await;
                *success_counter += 1;

                // You might want to make the resource return back to the pool more early by
                // calling drop(db)
                // Or if you don't it still able to return to the pool at the end of it's scope
                drop(db);
            });
        }
    });

    println!("Number of success requests are {}/{}", success_counter.read().await, number_of_running_tasks);
}
```

# Final thoughts


