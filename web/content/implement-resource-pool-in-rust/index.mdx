---
title: Resource Pooling in Rust
publishedDate: 2024-09-15
image: "../../public/images/image4.jpg"
isPublished: true
description: "Not init on the fly, cached but not too much instance, and not blocking the entire request when reached pool max. It is how we use and maintain our Http Connection, Web Socket, Database, gRPC connection"
authorDisplayName: "Tien Dang"
authorEmail: "tiendvlp@gmail.com"
authorFullName: "Dang Minh Tien"
keywords:
  - Rust
  - Design pattern
  - Resource pooling
  - Object pooling
---

# Why I'm writing this article ?

For any application to be able to run seamlessly over time, there are a lot of aspects that we have to cover, from memory safety to infrastructure. In this article, I would love to share a solution for an application that interacts with a limited source of resources. For example, 10,000 connections is the maximum number of concurrent connections to PostgreSQL, or 1024 is the default limit for concurrent opened files in Ubuntu. In that case, we cannot just ask for more and more resources as needed; we have to have a better strategy.

Moreover, interacting with these resources—whether through networking or mainly CPU-bound operations—produces some overhead if done excessively. Imagine if we make a new connection to the database on every RESTful request. It would be a disaster for our system; we would waste most of our CPU time initiating required resources instead of focusing on the main logic.

If I did touch your pain point(s), welcome to my article. The design pattern we will discuss today is Resource Pooling, using Rust and asynchronous syntax.

# Who Will Benefit Most from Reading This?

- **Developers looking for an implementation of resource pooling in Rust.**
- **Individuals seeking a resource pooling solution that works with asynchronous programming.**
- **High-expectation developers who not only want a solution but also desire the best experience while they are using.**

# The ideas

To start, let’s briefly examine how straightforward it is to use once implemented:

```rust
pub struct UserRepository {
    pub db_request: PoolRequest<SurrealDbConnection>
}

impl UserRepository {
    async fn find_all(&self) -> Result<Errors, Vec<User>> {
        let db = self.db_request.retreive().await; // Option<PoolResponse<DbConn>>
        if db.is_none() {
            return Err(Errors::DatabaseConnectionError)
        }                                           

        let db = db.unwrap();
        // The PoolResponse has been implement Deref trait
        // which allow us to access the resource function directly
        let db_results = db.query("SELECT * FROM Users").await;
        // Further process...
    }
}
```    
The `PoolRequest<T>` is a lightweight object that acts as a bridge to the `Pool`. It can be easily cloned and shared as we wish.

The `retrieve` function will request the pool to provide us a `PoolResponse<T>`.

Then we can use the `PoolResponse<T>` as it is our resource because it can be dereferenced into the resource.

This design could ensure both simplicity for using, and making sure the resource will always returned to the pool to be reused for other requests.

Take a look at this simple flow on how a basic API application will use the pool to solve resource initialization overhead by applying the Resource Pool pattern
(more complicated examples are provided at the end).

<div className='md:w-[100%]'>
  <Lottie className='aspect-[2/1.8] rounded-lg' file='dotlottie/resource-pool_-_simple-flow.lottie'/>
</div>

When a new request arrives, the request handler initializes a `PoolRequest<T>` that holds a connection to the pool. There’s no need to create a new resource; a `PoolRequest` is a simple struct that can be easily cloned across different asynchronous tasks.
The PoolRequest then acquires a resource from the pool for us to use freely, and once we’re finished, the resource automatically returns to the pool to wait for other requests.

# The problems and solutions
But by using the simple approach described above, four potential issues may arise:

#### 1. Inability to provide resources when the number of concurrent requests exceeds the number of resources in the pool.

   <ArrowRight/> To address this, we'll implement two limits. The first is `min_pool_size`, which defines the minimum number of resources that are always ready for use. The second is `max_pool_size`, which can be much larger than `min_pool_size`. This allows additional resources to be initialized to handle extra requests, but the total number of resources will not exceed `max_pool_size`.

#### 2. Applying `max_pool_size` could result in a rapid increase in the number of resources.

   <ArrowRight/> We'll introduce `resource_idle_timeout`, specifying the maximum time an unused resource can remain in memory while still maintaining the minimum number of resources in the pool.

   For example, if your RESTful API typically handles 10 concurrent requests and you set `min_pool_size = 10`, but suddenly receive 1,000 requests at once, your application will return to 10 concurrent requests after the surge, leaving 990 resources unused in the pool.

   By setting `resource_idle_timeout = 5 minutes`, the pool can release those 990 unused resources if they remain idle for five minutes.

   <Info>Note: The number of resources in the pool will always be `≥ min_pool_size`.</Info>

#### 3. What if requests exceed `max_pool_size`? Should we fail those requests?

   <ArrowRight/> We can handle this by applying `retrieving_timeout` to `PoolRequest<T>`. This allows the request to wait (within a specified timeout) for a resource to be released back into the pool, which can then be used for the current request.

#### 4. Setting a high `min_pool_size` might cause our service to take a long time to start because it needs to initialize many resources.

   <ArrowRight/> Instead of initializing resources one at a time, we can initialize them concurrently, reducing the startup time.

#### Below is the final diagram illustrating the flow of retrieving, releasing resources, and removing idle resources
<Lottie className='aspect-[1.3/1] rounded-lg' file='dotlottie/resource-pool_-_full-flow.lottie'/>

# Implementation

### Resource Abstraction

Before implementing the pool, we need a way to abstract the resources because the pool doesn't concern itself with what the resource is or how to initialize it. It only cares about the state of the resource—for example, whether it has timed out.

#### a. ResourceProvider

To abstract the process of creating new resources, we'll create a trait that encapsulates the initialization logic for different types of resources.

```rust
pub trait PoolResourceProvider<T>: Send + Sync
    where T: Send + Sync
{
    async fn new(&self) -> T where Self: 'async_trait;
}
```

You may notice that the new function cannot accept custom parameters, but it’s essential to initialize a resource with custom parameters.

No need to worry; we can create a struct to hold the parameters, and that struct will implement the `PoolResourceProvider<T>` trait.

```rust
pub struct DbPoolProvider {
  connection_string: string
}

impl PoolResourceProvider<PosgresConnection> for DbPoolProvider {
  async fn new(&self) -> PosgresConnection {
    let db = postgres.connect(self.connection_string).await;
    db
  }
}
```

Now that we’ve established an effective abstraction for creating new resources, let’s proceed to the next step.

#### b. PoolItem\<T>

The fundamental idea behind a PoolItem is that it represents a ‘slot’ for a resource. When there is a request, a resource can leave this slot
to become a `PoolResponse`, and after finishing its duty, it can return to the slot to wait for the next request to serve.

Let's take a look on struct `PoolItem<T>`

```rust
pub struct PoolItem<T>
where
    T: Send + Sync + 'static,
{
    resource: Option<T>,
    start_time: Instant,
    max_idling_timeout: Option<Duration>
}
```

#### Here are several things I'd like to explain:

#### 1. Why is the `resource` variable an `Option<T>`? Can the `PoolItem` be created without a resource?

No, it cannot. The `Option` type is used because the resource can be taken out to become a `PoolResponse<T>`, leaving the value as `None`. After the resource has completed its mission, it can return to the pool item, and the variable will be `Some(resource)` again.

#### 2. The `start_time` and `max_idling_timeout`

These are used to calculate the total idle time of a resource. If the resource doesn't belong to the `min_pool_size`, it will be dropped after it exceeds the `max_idling_timeout`.

Next, we will take a look at the implementation of the `PoolItem`:

```rust
impl<T> PoolItem<T> where T: Send + Sync + 'static
{
    pub async fn new(resource: T, max_idling_timeout: Option<Duration>) -> Self {
        Self {
            resource: Some(resource),
            start_time: Instant::now(),
            max_idling_timeout
        }
    }

    pub fn is_empty(&self) -> bool {
       self.resource.is_none() 
    }

    pub fn take(&mut self) -> Option<T> {
        self.resource.take()
    }

    pub fn put(&mut self, resource: T) {
        self.resource = Some(resource);
        self.start_time = Instant::now();
    }

    pub fn invalidate(&mut self) -> bool {
        if self.max_idling_timeout.is_some() && self.resource.is_some() {
            let elapsed = self.start_time.elapsed();
            if elapsed.gt(self.max_idling_timeout.as_ref().unwrap()) {
                drop(self.resource.take());
                return true;
            }
        }

        return false;
    }
}
```

As mentioned earlier, the `PoolItem` cannot be created without a resource; therefore, the `::new` function must require the resource to be `Some`.
The `max_idling_timeout` is optional because we don't want to apply a timeout to resources within the `min_pool_size`, the pool must always contain minimum number
of pool item equal the `min_pool_size`.

The `::take` function is triggered by the pool to extract the resource, leaving the `resource` variable as `None`.

Conversely, the `::put` function returns the resource to the `PoolItem` after it has finished its duty.

The `::invalidate` function will drop the resource if it hasn't been used within the `max_idling_timeout`.
I'll explain more about the logic that calls this `drop` function, which is managed by the `PoolCleaner`.

### The Pool

Finally, let's discuss our main component: the pool itself.

The pool consists of two primary parts:

**Allocator**: This component initializes resources and provides them as needed. It handles the creation of new resources and supplies them to requesters.

**Cleaner**: This part is responsible for maintaining the pool's efficiency by cleaning up unused resources, preventing unnecessary memory usage.

#### a. PoolAllocator\<T>

First let's take a look on the allocator.

```rust
pub struct PoolAllocator<T>
where
    T: 'static + Send + Sync
{
    pool: Arc<RwLock<Vec<PoolItem<T>>>>,
    min_pool_size: u16,
    max_pool_size: u16,
    resource_idle_timeout: Duration,
    resource_provider: Box<dyn PoolResourceProvider<T> + 'static>,
    pool_cleaner: PoolCleaner<T>
}
```

We create a `PoolAllocator` struct that requires a generic type `<T>`, representing the type of resource it will provide.

The `PoolAllocator<T>` holds a `Vector` of `PoolItem<T>`, which is wrapped inside an `Arc` to allow sharing the reference across different threads. We then wrap this with an `RwLock`, enabling multiple readers but only one writer at a time. Together, these wrappers allow us to share the `PoolAllocator` across threads without violating Rust's memory safety rules.

Regarding why the allocator needs to keep a reference to the `PoolCleaner`, it's because it decides when to start the cleanup.
Additionally, this ensures that the `PoolCleaner<T>` will be dropped when the allocator is dropped.

Now, let's examine the main logic of the allocator to see how it works.

```rust
impl<T> PoolAllocator<T> where T: Send + Sync + 'static
{
    pub async fn init(&mut self)
    {
        tokio_scoped::scope(|scope| {
            for _i in 0..self.min_pool_size {
                let pool = self.pool.clone();
                let resource_provider = &self.resource_provider;
                scope.spawn(async move {
                    let resource = resource_provider.new().await;
                    let pool_item: PoolItem<T> = PoolItem::new(resource, None).await;

                    let mut pool = pool.write().await;
                    pool.push(pool_item);
                });
            }
        });

        let pool = self.pool.read().await;
        info!(
            target: "pool-allocator",
            "Initialized {} resources to fit the min_pool_size={}",
            pool.len(),
            self.min_pool_size
        );

        self.pool_cleaner.start_background();
    }

    pub async fn len(&self) -> usize {
        let pool = self.pool.read().await;
        pool.len()
    }

    pub async fn retrieve(&self) -> Option<T> {
         let mut pool = self.pool.write().await;
         let size = pool.len();

         let found_pool_item = pool.iter_mut().find(|it| !it.is_empty());
         if found_pool_item.is_some() {
            return found_pool_item.unwrap().take()
         }
         else if size < self.max_pool_size as usize {
            let resource = self.resource_provider.new().await;
            let mut new_item = PoolItem::<T>::new(
                resource,
                Some(self.resource_idle_timeout))
                .await;

            let resource = new_item.take();
            pool.push(new_item);

            return resource
         }

         None
    }

    pub async fn put(&self, resource: T) {
        let mut pool = self.pool.write().await;
        let found_empty_item = pool.iter_mut().find(|it| it.is_empty());
        if found_empty_item.is_some() {
            found_empty_item.unwrap().put(resource);
        }
        else {
            // Every resource when complete, it will always have place to return to the pool
            error!(target: "pool-allocator", "Found an orphan resource, dropping it...");
         }
    }
}
````

The `::init` function has two main responsibilities. First, it initializes the minimum number of resources to populate the pool. And to prevent long startup time
we will create all resources concurrently instead of one by one, and notice that these minimum resources will not have `max_idling_timeout`.
Second, it initiates the cleanup process, which runs in the background to monitor the pool and remove unused resources.

The `::retrieve` function extracts a resource from the pool. If no resource is available, it checks the current number of resources in the pool. If this number is still less than the `max_pool_size`, the allocator will initialize additional resources, and these extra resources will has `max_idling_timeout`.
Finally, the new resource will be used to serve the request.

The `::put` function returns the resource to the pool. It's important to note that the pool will only accept the resource if there is an available slot (`PoolItem.resource = None`). And if no available slot is found, the resource will be dropped, and it will never happen, unless there is a resource that not comming from the pool.

#### b. PoolCleaner\<T>

We have successfully implemented resource allocation; now it's time to free up resources after they've been used.
To clean up unused resources, we track the idle time of any resource that doesn't belong to the `min_pool`. 
If a resource's idle time exceeds the `resource_idle_timeout`, we remove it.

<Info>
  A resource is considered idle when it isn't being requested and only laying within the `PoolItem`.
</Info>

Let's take a quick look at how this process works:

```rust
pub enum CleanupStrategy {
    Relax { interval: Duration }
}

pub struct PoolCleaner<T>
where
    T: Send + Sync + 'static
{
    strategy: CleanupStrategy,
    pool: Arc<RwLock<Vec<PoolItem<T>>>>,
    background_handler: Option<JoinHandle<()>>
}
```


The CleanupStrategy enum exists because cleaning up resources on time can be challenging and may require high CPU usage.
Therefore, it’s better to apply different approaches for different types of resources. 
Currently, I only only propose one simple cleanup approach called Relax.
In this mode, we start a background job that performs the cleanup task at each specified interval. 
It’s called Relax because it might not clean up resources exactly on time and may have a slight delay.
You might want to consider another approach to suit your specific cases.

The implementation

```rust
impl<T> PoolCleaner<T>
where
    T: Send + Sync + 'static,
{
    pub fn new(pool: Arc<RwLock<Vec<PoolItem<T>>>>, strategy: CleanupStrategy) -> Self {
        Self {
            strategy,
            pool,
            background_handler: None
        }
    }

    pub fn start_background(&mut self) {
        match self.strategy {
            CleanupStrategy::Relax {
                interval
            } => {
                let interval = interval.clone();
                let pool = self.pool.clone();
                let handler = spawn(async move {
                    loop {
                        sleep(interval).await;
                        Self::request_cleanup_loop(&pool).await
                    }
                });

                self.background_handler = Some(handler);
            }
            _ => {
                // Not support for other task
            }
        }
    }

    pub fn stop_background(&mut self) {
        if self.background_handler.is_some() {
            self.background_handler.take().unwrap().abort();
        }
    }

    pub async fn request_cleanup_loop(pool_sync: &Arc<RwLock<Vec<PoolItem<T>>>>) {
        let mut pool = pool_sync.write().await;

        pool.retain_mut(|it| !it.invalidate());
    }
}

impl<T> Drop for PoolCleaner<T>
where
    T: Send + Sync + 'static,
{
    fn drop(&mut self) {
        self.stop_background();
    }
}
```

Let's begin with the key functionality: the `request_cleanup_loop` function. This function calls `invalidate` on each item, allowing them to determine how to clean up their resource.
The returned value, `true` or `false`, informs the cleaner whether an item has completed its cleanup and can be removed from the pool.

The `start_background` function is triggered by the pool within the `init` function. If you'd prefer to use a cleanup strategy that doesn't require a background handler, you can simply leave the match arm empty within this function.

### The PoolRequest\<T>

The `PoolAllocator` is great, but it doesn’t seem as great when we have to share its reference across various async tasks and modules.
We all know how tough Rust can be, right?

That’s where the `PoolRequest` comes in to help us manage Rust’s memory safety rules.

Additionally, the PoolRequest will handle the logic of waiting for available resources when the PoolAllocator has none left.

Let’s take a look at how it works:

```rust
pub struct PoolRequest<T>
where
    T: Send + Sync + 'static,
{
    retreving_timeout: Option<Duration>,
    pool: Arc<PoolAllocator<T>>
}
```

First, we need to agree that in order to share the same `PoolAllocator` instance across multiple async tasks, we have to place it on the heap instead of the stack.
Then, we wrap it in an `Arc`, which allows us to clone and share the instance while ensuring thread safety across different threads.

Now, let's move on to its implementation:

```rust
impl<T> PoolRequest<T>
where
    T: Send + Sync + 'static
{
    pub async fn retreive(&self) -> Option<PoolResponse<T>> {
        let resource = self.pool.retrieve().await;

        if resource.is_none() {
            if self.retreving_timeout.is_none() {
                return None
            }

            let now = Instant::now();
            let mut elapsed = Duration::new(0, 0);

            while elapsed.lt(self.retreving_timeout.as_ref().unwrap()) {
                let resource = self.pool.retrieve().await;
                if resource.is_some() {
                    return Some(PoolResponse::new(resource.unwrap(), self.pool.clone()));
                }

                elapsed = now.elapsed();
            }

            return None
        }

        let response = PoolResponse::new(resource.unwrap(), self.pool.clone());

        Some(response)
    }
}
```

What the `retreive` function is doing is that it will call the `take` function from the pool, and if there is no available resource, it will create a loop to continuously asking the resource
from the pool within the `retreiving_timeout`.

There is not much thing to talk about this `retreive` function, let's explain why we need to return the `PoolResponse` instead of a resource.

### The PoolResponse\<T>

You might ask, why do we have to use it? 
To be honest, I would prefer the `PoolRequest` to return the resource directly.
We all know how it feels to have ownership of an instance rather than just a reference, right?

But every resource has to go back to the pool to be reused by the others ! What if I, you, or one of our colleagues takes the resource and just drops it? It would never return to the pool, and it would never be reused as it should be.
We need a better approach. The `PoolResponse` ensures the resource is always returned to the pool.

Now, let's take a look at how it works.

```rust
pub struct PoolResponse<T> where T: Send + Sync + 'static
{
    resource: Option<T>,
    pool: Option<Arc<PoolAllocator<T>>>
}
```

It's easy to understand why the `PoolResponse` needs to hold a reference to the `PoolAllocator`: Because it must know which pool it need to return back after use.

Another thing is that both the resource and pool variables are Option; both are required to be `Some(thing)` when initialized.
We’ll understand how could it become `None` in this implementation.

```rust
impl<T> PoolResponse<T> where T: Send + Sync + 'static
{
    pub fn new(resource: T, pool: Arc<PoolAllocator<T>>) -> Self {
        Self {
            resource: Some(resource),
            pool: Some(pool)
        }
    }
}

impl<T> Deref for PoolResponse<T> where T: Send + Sync + 'static,
{
    type Target = T;

    fn deref(&self) -> &Self::Target {
        self.resource.as_ref().expect("Cannot access returned resource")
    }
}

impl<T> DerefMut for PoolResponse<T> where T: Send + Sync + 'static
{
    fn deref_mut(&mut self) -> &mut Self::Target {
        self.resource.as_mut().expect("Cannnot access the returned resource")
    }
}

impl<T> Drop for PoolResponse<T> where T: Send + Sync + 'static
{
    fn drop(&mut self) {
        let pool = self.pool.take();
        let resource = self.resource.take();

        spawn(async move {
            pool.expect("This response already dropped")
                .put(resource.expect("The response already dropped"))
                .await;
        });
    }
}
```

As I already mentioned above, the `PoolResponse::new` function will not accept `None` from either the resource or the pool.

By implementing both `Deref` and `DerefMut`, we are able to freely interact with the resource without any limitations. We can also call `expect` without any hesitation because it cannot fail; the pool and resource will only be `None` when it is dropped.

And the secret on how the resource could automatically return to the pool after used is within the `Drop` trait implementation. Whenever the `PoolResponse` goes out of scope or we manually call `drop(response)`, we will spawn an async task that returns the resource back to the pool. This also explains why the pool and resource need to be an `Option`, because we need to take them out and pass them to the async task, which runs independently after the `PoolResponse` is dropped.

# Usage

# Final thoughts

