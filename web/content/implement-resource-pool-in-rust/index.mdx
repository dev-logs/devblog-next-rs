---
title: Resource Pooling in Rust
publishedDate: 2024-09-15
image: "../../public/images/image4.jpg"
isPublished: true
description: "Not init on the fly, cached but not too much instance, and not blocking the entire request when reached pool max. It is how we use and maintain our Http Connection, Web Socket, Database, gRPC connection"
authorDisplayName: "Tien Dang"
authorEmail: "tiendvlp@gmail.com"
authorFullName: "Dang Minh Tien"
keywords:
  - Rust
  - Design pattern
  - Resource pooling
  - Object pooling
---

# Why I'm writing this article ?

I think any application that interact with resource like database, files, or http/websocket connection to other service will alway 
need to make sure two things:

1. Avoiding overhead in initiating resource.

2. Limit the concurrent number of resources we are using. There are no inifinite number of resources, for example the default maximum number of concurrent open files in Ubuntu are 1024 files, you can modify the number but still have to have a limit.
So that if we intend to develop an application works well overtime we need to be respect the limit
of resources we are interacting with.


# Who will receive the most benefit by reading this?
- Someone looking for an implementation of Resource Pooling in Rust.
- Someone looking for a Resource Pooling solution which works with asynchronous syntax.
- High expectation developers, who not only want a solution but also want a best experience while using it. I also put code with unit testing included in the end of this article.

Without more talk, let's jump into the implementation.

# The ideas
First let's look on how simple it is to use it in the end:

The `PoolRequest<T>` is a light weight object that acts as a bridge to the `Pool`, it could be easily to clone and share as we wish.
the `retreive` function will request the pool to give us a resource.
A resource will be wrapped in a `PoolResponse<T>` which can be deref into `T` which also the Resource type. Why we need it ?
It will make sure and simplify the process of alway return the resource to the pool, it allow the resource to be reused by other requests.

Take a look on this simple flow on how a simple API application will use Pool to solve resource inititaing overhead by applying Resource Pool pattern (more complicated at the end).
<div className='md:w-[100%]'>
  <Lottie className='aspect-[2/1.8] rounded-lg' file='dotlottie/resource-pool_-_simple-flow.lottie'/>
</div>

When a new request come, the Request Handler will inititate a `PoolRequest<T>` which holds a connection to the `PoolAllocator<T>`.
The handler will able to request a resource via PoolRequest instance, the result will be a `PoolResponse<T>` which could be deref to the resource, after `PoolResponse` is dropped it will take the resource back to the Pool, make sure every resource is reusable.

# The problems and solutions
With the simple approach above, there are four problems that could arise:

#### 1. Not able to provide resource if the number of concurrent requests is greater than the number of resources in the Pool.

   <ArrowRight/> We will have two limit, the first limit is `min_pool_size` which will define the minimum number of resources that will always ready to be used.
   The second is `max_pool_size` which could be alot bigger than the `min_pool_size`, it allow some extra resources to be 
 init to serve extra requests, but after all the total number of resources must be smaller than the `max_pool_size`.

#### 2. If we apply `max_pool_size`, the number of resources could be very large after a short time.

     <ArrowRight/> We will define `resource_idle_timeout` which will define the maximum time that an unused resource could remain on memory.
     While still keep the minimum number of resources within the pool.

    For example: Your Restful Api mostly only receives 10 concurrent requests at a time so that you set the `min_pool_size = 10`, but at a random time, it suddently receive 1000 requests at once.
    The things is after that hard time, your application is back to 10 concurrent requests at a time,
    we could left 990 resources as unused.

    If we set `resource_idle_timeout = 5 minutes`, the Pool could releasing 990 unused resources if it not in used for 5 minutes. 

    <Info>Note that the minimum number of resources in Pool will always be `>= min_pool_size`</Info>

#### 3. What if the request exceed the `max_pool_size` ? Are we going to panic that request ?
    
    <ArrowRight/> We have another chance, we could apply `retreiving_timeout` which will be apply for the `PoolRequest<T>`, it will wait (within timeout) for another request to released the resource to the pool and that resource will be used for the current request.

#### 4. If we apply `min_pool_size` it could take very long for our service to start because it have to init large number of resources.

    <ArrowRight/> Instead of init single resource at once, we could init all of them concurrently, by that, it could reduce the initial time.

#### Here is the final diagram which demonstrates the flow of retreiving, releasing resources, and removing the idle resources

<div className='md:w-[110%]'>
  <Lottie className='aspect-[1.3/1] rounded-lg' file='dotlottie/resource-pool_-_full-flow.lottie'/>
</div>

# Implementation

### Resource abstraction

Before we actually implement the Pool, let provide a way to abstract the resource because the pool don't care what is the resource or how to init it. It only care about the state of the resource, for example the resource has been timeout or not. 

#### a. ResourceProvider

Abstract the initialize process for the resource, the `Pool` will only need to call `::new().await` without worrying about how to each type of resource.

#### b. PoolItem\<T>

Wrap the resource and manage resource state, giving ability to verify the resource has exceed the timeout or not.

### The Pool

#### a. PoolAllocator\<T>

After finished the implementation of the `PoolItem<T>` we finally able to implement our lead actor, the `PoolAllocator<T>`

As the code above, the `PoolAllocator<T>` holds an a `Vector` of `PoolItem<T>`, it is wrapped with an `Arc` which allow us to share the reference of the vector across different threads.
Then, we continue to wrap it using the `RwLock` which allow the `Vector` to have multiple reader but only allow one writer at the same time. Together, they allowing us to share the `PoolAllocator` across different threads
without breaking the rule of Rust in memory safety.

Keeping the reference to the `PoolCleanup` will give more control to the `PoolAllocator` on when to cleanup it self by calling `cleanup.request_cleanup_loop()`
more over we can make sure the PoolCleanup will also dropped when PoolAllocator dropped.

#### b. PoolCleaner\<T>


### The Request
#### a. PoolRequest\<T>

As you can see, the `PoolRequest<T>` is able to take out the resource from the Pool, or if no resource available, it will wait within timeout until some one return the resource back to the pool.
And by clonning the `PoolRequest<T>` when ever we need, instead of init new resource or manage the reference across async task (which is very hard) we can take the entire ownership of Pool Request which is much easy to handle.

#### b. PoolResponse\<T>

The `PoolResponse<T>` is being returned by the `PoolRequest<T>` and can be deref into the resource it self, by doing this way, we make sure whenever the `PoolResponse` is no longer in used or being dropped manually by calling `drop()` it will always return the resource back to the Pool. We are no longer worry about the forgotten of returning back the resource.

# Usage

# Final thoughts

